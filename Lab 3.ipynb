{"cells":[{"cell_type":"markdown","metadata":{"id":"X6WDvajSqIDs"},"source":["# Lab 3: Gesture Recognition using Convolutional Neural Networks\n","\n","In this lab you will train a convolutional neural network to make classifications on different hand gestures. By the end of the lab, you should be able to:\n","\n","1. Load and split data for training, validation and testing\n","2. Train a Convolutional Neural Network\n","3. Apply transfer learning to improve your model\n","\n","Note that for this lab we will not be providing you with any starter code. You should be able to take the code used in previous labs, tutorials and lectures and modify it accordingly to complete the tasks outlined below.\n","\n","### What to submit\n","\n","Submit a PDF file containing all your code, outputs, and write-up\n","from parts 1-5. You can produce a PDF of your Google Colab file by\n","going to **File > Print** and then save as PDF. The Colab instructions\n","has more information. Make sure to review the PDF submission to ensure that your answers are easy to read. Make sure that your text is not cut off at the margins.\n","\n","**Do not submit any other files produced by your code.**\n","\n","Include a link to your colab file in your submission.\n","\n","Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission."]},{"cell_type":"markdown","metadata":{"id":"LfiFE_WOqIDu"},"source":["## Colab Link\n","\n","Include a link to your colab file here\n","\n","Colab Link: https://colab.research.google.com/drive/1WtDzrUv3uSd5FbHvej-iGRkgYgP3yOhm?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"kvTXpH_kqIDy"},"source":["## Dataset\n","\n","American Sign Language (ASL) is a complete, complex language that employs signs made by moving the\n","hands combined with facial expressions and postures of the body. It is the primary language of many\n","North Americans who are deaf and is one of several communication options used by people who are deaf or\n","hard-of-hearing. The hand gestures representing English alphabet are shown below. This lab focuses on classifying a subset\n","of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand\n","showing one of the letters A-I, we want to detect which letter is being represented.\n","\n","![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)"]},{"cell_type":"markdown","metadata":{"id":"bJxMgWGNqID2"},"source":["## Part B. Building a CNN [50 pt]\n","\n","For this lab, we are not going to give you any starter code. You will be writing a convolutional neural network\n","from scratch. You are welcome to use any code from previous labs, lectures and tutorials. You should also\n","write your own code.\n","\n","You may use the PyTorch documentation freely. You might also find online tutorials helpful. However, all\n","code that you submit must be your own.\n","\n","Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unecessary\n","for loops, or unnecessary calls to unsqueeze()). Ensure enough comments are included in the code so that\n","your TA can understand what you are doing. It is your responsibility to show that you understand what you\n","write.\n","\n","**This is much more challenging and time-consuming than the previous labs.** Make sure that you\n","give yourself plenty of time by starting early."]},{"cell_type":"markdown","metadata":{"id":"MiDuQaAh56sT"},"source":["### 1. Data Loading and Splitting [5 pt]\n","\n","Download the anonymized data provided on Quercus. To allow you to get a heads start on this project we will provide you with sample data from previous years. Split the data into training, validation, and test sets.\n","\n","Note: Data splitting is not as trivial in this lab. We want our test set to closely resemble the setting in which\n","our model will be used. In particular, our test set should contain hands that are never seen in training!\n","\n","Explain how you split the data, either by describing what you did, or by showing the code that you used.\n","Justify your choice of splitting strategy. How many training, validation, and test images do you have?\n","\n","For loading the data, you can use plt.imread as in Lab 1, or any other method that you choose. You may find\n","torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=image%20folder#torchvision.datasets.ImageFolder\n",")"]},{"cell_type":"code","source":["# mount our Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!unzip '/content/drive/My Drive/Colab Notebooks/Lab3_Data.zip' -d '/root/datasets'\n","\n","import time\n","import os\n","import numpy as np\n","import torch\n","\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# I chose to split the data manually by creating 3 folders labelled\n","# train, validation and test with around 70%, 15% and 15% data respectively.\n","# The 3 folders are in the main directory and they each include the class\n","# folders. I chose to do it manually as that is what made most sense\n","# to me, for the rest of the code, I followed tutorial 3b.\n","\n","# define training, validation and test data directories\n","data_dir = '/root/datasets/Lab3_Gestures_Summer'\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir = os.path.join(data_dir, 'validation')\n","test_dir = os.path.join(data_dir, 'test')\n","\n","# classes are folders in each directory with these names\n","classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n","\n","# load and transform data using ImageFolder\n","\n","# resize all images to 224 x 224\n","data_transform = transforms.Compose([transforms.Resize((224, 224)),\n","                                      transforms.ToTensor(),\n","                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize with mean and standard deviation\n","                                     ])\n","\n","train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n","val_data = datasets.ImageFolder(val_dir, transform=data_transform)\n","test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n","\n","# print out some data stats\n","print('Num training images: ', len(train_data))\n","print('Num validation images: ', len(val_data))\n","print('Num testing images: ', len(test_data))\n","\n","# define dataloader parameters\n","batch_size  = 20\n","num_workers = 0\n","\n","# prepare data loaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                           num_workers=num_workers, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","\n","# Visualize some sample data\n","\n","# obtain one batch of training images\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","images = images.numpy() # convert images to numpy for display\n","\n","# plot the images in the batch, along with the corresponding labels\n","fig = plt.figure(figsize=(25, 4))\n","for idx in np.arange(20):\n","    ax = fig.add_subplot(2, int(20/2), idx+1, xticks=[], yticks=[])\n","    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n","    ax.set_title(classes[labels[idx]])\n"],"metadata":{"id":"dosvBIXgDDTX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67a2a2ac-1942-406b-e813-a98834c42033"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Archive:  /content/drive/My Drive/Colab Notebooks/Lab3_Data.zip\n","replace /root/datasets/Lab3_Gestures_Summer/small_dataset/test/A/568_A_2.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","metadata":{"id":"5VWX4DGY5gQE"},"source":["### 2. Model Building and Sanity Checking [15 pt]\n","\n","### Part (a) Convolutional Network - 5 pt\n","\n","Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture\n","letter. Your model should be a subclass of nn.Module. Explain your choice of neural network architecture: how\n","many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional?\n","What about other decisions like pooling layers, activation functions, number of channels / hidden units?"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2dtx1z5951fS","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.name = \"CNN\"\n","        self.conv1 = nn.Conv2d(3, 16, 4) #there are 3 inputs, RGB\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(16, 32, 4) #output of conv1 is input of conv2\n","        self.conv3 = nn.Conv2d(32, 64, 4) #same as above\n","        self.fc1 = nn.Linear(64 * 25 * 25, 512) #output from conv3 flattened\n","        self.fc2 = nn.Linear(512, 9) #output is 9 cause 9 classes\n","        #there are 3 convolution that are used to extract spatial\n","        #features from the input images and 2 fully connected layers\n","        #that perform classification based on the learned features\n","        #from the convolution layers. There is a max pooling layer\n","        #applied after each convolution layer to downsample the\n","        #feature maps and reduce spatial dimensions while\n","        #mantaining important features. The hidden units and\n","        #number of channels was chosen through research\n","        #to choose numbers that are simple and effective for the task.\n","        #The values for the first linear layer were found using the\n","        #equation from tutorial 3a.\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) #the reLU activation function is\n","        x = self.pool(F.relu(self.conv2(x))) #simple and effective\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 25 * 25)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        x = x.squeeze(1) # Flatten to [batch_size]\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"XeGvelvb515e"},"source":["### Part (b) Training Code - 5 pt\n","\n","Write code that trains your neural network given some training data. Your training code should make it easy\n","to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure\n","that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice\n","of loss function and optimizer."]},{"cell_type":"code","source":["def evaluate(net, loader, criterion):\n","    \"\"\" Evaluate the network on the validation set.\n","\n","     Args:\n","         net: PyTorch neural network object\n","         loader: PyTorch data loader for the validation set\n","         criterion: The loss function\n","     Returns:\n","         err: A scalar for the avg classification error over the validation set\n","         loss: A scalar for the average loss function over the validation set\n","     \"\"\"\n","    total_loss = 0.0\n","    total_correct = 0.0\n","    total_samples = 0\n","    for i, data in enumerate(loader, 0):\n","        inputs, labels = data\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        _, predicted = torch.max(outputs, 1)\n","        total_correct += (predicted == labels).sum().item()\n","        total_loss += loss.item() * inputs.size(0)\n","        total_samples += inputs.size(0)\n","    err = 1.0 - (total_correct / total_samples)\n","    loss = total_loss / total_samples\n","    return err, loss"],"metadata":{"id":"V0yBtYzy8wBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_model_name(name, batch_size, learning_rate, epoch):\n","    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n","\n","    Args:\n","        config: Configuration object containing the hyperparameters\n","    Returns:\n","        path: A string with the hyperparameter name and value concatenated\n","    \"\"\"\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n","                                                   batch_size,\n","                                                   learning_rate,\n","                                                   epoch)\n","    return path"],"metadata":{"id":"VC1HkcgN8rS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"17YTQv4l54W1","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["def train_net(train_loader, val_loader, test_loader, net, batch_size=32, learning_rate=0.001, num_epochs=10):\n","    ########################################################################\n","    # Train a classifier\n","    target_classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n","    ########################################################################\n","    # Fixed PyTorch random seed for reproducible result\n","    torch.manual_seed(1000)\n","    ########################################################################\n","    # Define the Loss function and optimizer\n","    #I decided to use cross entropy as it was used in tutorial for multiclassification\n","    #and was suggested in the lecture as a good non-regresion loss function for\n","    #multiple classes. For the optimizer, I chose Adam as it is generally more effective\n","    #than SGD.\n","    train_loss = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    train_err = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","    ########################################################################\n","\n","    ########################################################################\n","    # Train the network\n","    # Loop over the data iterator and sample a new batch of training data\n","    # Get the output from the network, and optimize our loss function.\n","    start_time = time.time()\n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","        total_train_loss = 0.0\n","        total_train_err = 0.0\n","        total_epoch = 0\n","        for i, data in enumerate(train_loader, 0):\n","            # Get the inputs\n","            inputs, labels = data\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","            # Forward pass, backward pass, and optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            # Calculate the statistics\n","            total_train_err += torch.sum(outputs.argmax(dim=1) != labels).item()\n","            total_train_loss += loss.item()\n","            total_epoch += len(labels)\n","        train_err[epoch] = total_train_err / total_epoch\n","        train_loss[epoch] = total_train_loss / (i+1)\n","        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n","        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n","               \"Validation err: {}, Validation loss: {}\").format(\n","                   epoch + 1,\n","                   train_err[epoch],\n","                   train_loss[epoch],\n","                   val_err[epoch],\n","                   val_loss[epoch]))\n","        # Save the current model (checkpoint) to a file\n","        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n","        torch.save(net.state_dict(), model_path)\n","    print('Finished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","    # Write the train/test loss/err into CSV file for plotting later\n","    epochs = np.arange(1, num_epochs + 1)\n","    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n","    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n","    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n","    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"]},{"cell_type":"markdown","metadata":{"id":"bk1RNgAj54rZ"},"source":["### Part (c) “Overfit” to a Small Dataset - 5 pt\n","\n","One way to sanity check our neural network model and training code is to check whether the model is capable\n","of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code\n","should be able to memorize the answers to a small number of images quickly.\n","\n","Construct a small dataset (e.g. just the images that you have collected). Then show that your model and\n","training code is capable of memorizing the labels of this small data set.\n","\n","With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, You should be\n","able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations)."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lXYRBhQO6d3u","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# define training, validation and test data directories\n","data_tiny_dir = '/root/datasets/Lab3_Gestures_Summer/small_dataset'\n","train_tiny_dir = os.path.join(data_tiny_dir, 'train')\n","val_tiny_dir = os.path.join(data_tiny_dir, 'validation')\n","test_tiny_dir = os.path.join(data_tiny_dir, 'test')\n","\n","# load and transform data using ImageFolder\n","\n","train_tiny_data = datasets.ImageFolder(train_tiny_dir, transform=data_transform)\n","val_tiny_data = datasets.ImageFolder(val_tiny_dir, transform=data_transform)\n","test_tiny_data = datasets.ImageFolder(test_tiny_dir, transform=data_transform)\n","\n","# print out some data stats\n","print('Num training images: ', len(train_tiny_data))\n","print('Num validation images: ', len(val_tiny_data))\n","print('Num testing images: ', len(test_tiny_data))\n","\n","\n","# prepare data loaders\n","train_tiny_loader = torch.utils.data.DataLoader(train_tiny_data, batch_size=batch_size,\n","                                           num_workers=num_workers, shuffle=True)\n","val_tiny_loader = torch.utils.data.DataLoader(val_tiny_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","test_tiny_loader = torch.utils.data.DataLoader(test_tiny_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n"]},{"cell_type":"code","source":["def plot_training_curve(path):\n","    \"\"\" Plots the training curve for a model run, given the csv files\n","    containing the train/validation error/loss.\n","\n","    Args:\n","        path: The base path of the csv files produced during training\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n","    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n","    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n","    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n","    plt.title(\"Train vs Validation Error\")\n","    n = len(train_err) # number of epochs\n","    plt.plot(range(1,n+1), train_err, label=\"Train\")\n","    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Error\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    plt.title(\"Train vs Validation Loss\")\n","    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()"],"metadata":{"id":"_U5Yj47XNEps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = CNN()\n","train_net(train_tiny_loader, val_tiny_loader, test_tiny_loader, net, num_epochs=15)\n","model_path = get_model_name(\"CNN\", batch_size=32, learning_rate=0.001, epoch=14)\n","plot_training_curve(model_path)"],"metadata":{"id":"B1nic-FwM1Mh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvDLw-Vz6eVS"},"source":["### 3. Hyperparameter Search [15 pt]\n","\n","### Part (a) - 3 pt\n","\n","List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to\n","the model architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"HXRQbgMqR_Qy","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#batch size, learning rate and number of hidden layers. The number of hidden laters is related\n","#to the model architecture."]},{"cell_type":"markdown","metadata":{"id":"zeD6EzPB6kSW"},"source":["### Part (b) - 5 pt\n","\n","Tune the hyperparameters you listed in Part (a), trying as many values as you need to until you feel satisfied\n","that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"UkvdR-cB6nzm","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["net1 = CNN()\n","train_net(train_loader, val_loader, test_loader, net1, num_epochs=15)\n","model_path = get_model_name(\"CNN\", batch_size=32, learning_rate=0.001, epoch=14)\n","plot_training_curve(model_path)"]},{"cell_type":"code","source":["#I changed the number of epochs since the data was still being overfit\n","#and the training error was low within 9 epochs\n","net2 = CNN()\n","#increase the learning rate from 0.001 to 0.01\n","train_net(train_loader, val_loader, test_loader, net2, learning_rate=0.01, num_epochs=9)\n","model_path = get_model_name(\"CNN\", batch_size=32, learning_rate=0.01, epoch=8)\n","plot_training_curve(model_path)"],"metadata":{"id":"MAPK15zJqX03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net3 = CNN()\n","#increase the batch size from 32 to 64\n","train_net(train_loader, val_loader, test_loader, net3, batch_size=64, num_epochs=9)\n","model_path = get_model_name(\"CNN\", batch_size=64, learning_rate=0.001, epoch=8)\n","plot_training_curve(model_path)"],"metadata":{"id":"I63a4sREqbir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN_W(nn.Module):\n","    def __init__(self):\n","        super(CNN_W, self).__init__()\n","        self.name = \"CNN_W\"\n","        self.conv1 = nn.Conv2d(3, 16, 4) #there are 3 inputs, RGB\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(16, 32, 4) #output of conv1 is input of conv2\n","        self.conv3 = nn.Conv2d(32, 64, 4) #same as above\n","        self.fc1 = nn.Linear(64 * 25 * 25, 512) #output from conv3 flattened\n","        self.fc2 = nn.Linear(512, 256) #new hidden layer\n","        self.fc3 = nn.Linear(256, 9) #output is 9 cause 9 classes\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) #the reLU activation function is\n","        x = self.pool(F.relu(self.conv2(x))) #simple and effective\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 25 * 25)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = x.squeeze(1) # Flatten to [batch_size]\n","        return x"],"metadata":{"id":"a1b0ErndqhIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net4 = CNN_W()\n","train_net(train_loader, val_loader, test_loader, net4, num_epochs=9)\n","model_path = get_model_name(\"CNN_W\", batch_size=32, learning_rate=0.001, epoch=8)\n","plot_training_curve(model_path)"],"metadata":{"id":"7ETQjw0pqfXw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H93iN5_l60BO"},"source":["### Part (c) - 3 pt\n","Choose the best model out of all the ones that you have trained. Justify your choice."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kIhcN7IG6zRO","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#I would choose the last model with an extra hidden layer since it overfits the data the least.\n","#But to increase accuracy I would also increase the batch size to 64 since that resulted in\n","#the highest accuarcy between all the models.\n","\n","net5 = CNN_W()\n","train_net(train_loader, val_loader, test_loader, net5, num_epochs=9, batch_size=64)\n","model_path = get_model_name(\"CNN_W\", batch_size=64, learning_rate=0.001, epoch=8)\n","plot_training_curve(model_path)"]},{"cell_type":"markdown","metadata":{"id":"QzNA5oup67JO"},"source":["### Part (d) - 4 pt\n","Report the test accuracy of your best model. You should only do this step once and prior to this step you should have only used the training and validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2eJ7AbVl6-ax","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","error, loss = evaluate(net5, test_loader, criterion)\n","print(\"Test Error:\", error, \"Test Loss:\", loss)"]},{"cell_type":"markdown","metadata":{"id":"Wrem-iXV6_Bz"},"source":["### 4. Transfer Learning [15 pt]\n","For many image classification tasks, it is generally not a good idea to train a very large deep neural network\n","model from scratch due to the enormous compute requirements and lack of sufficient amounts of training\n","data.\n","\n","One of the better options is to try using an existing model that performs a similar task to the one you need\n","to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed **Transfer\n","Learning**. In this assignment, we will use Transfer Learning to extract features from the hand gesture\n","images. Then, train a smaller network to use these features as input and classify the hand gestures.\n","\n","As you have learned from the CNN lecture, convolution layers extract various features from the images which\n","get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal\n","role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an\n","ImageNet pre-trained AlexNet model to extract features in this assignment."]},{"cell_type":"markdown","metadata":{"id":"rWdQJz4Q7O2F"},"source":["### Part (a) - 5 pt\n","Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch\n","will download the pretrained weights from the internet."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BJKcTW9C7TZk","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["import torchvision.models\n","alexnet = torchvision.models.alexnet(pretrained=True)"]},{"cell_type":"markdown","metadata":{"id":"NQ0GZYaP7VAR"},"source":["The alexnet model is split up into two components: *alexnet.features* and *alexnet.classifier*. The\n","first neural network component, *alexnet.features*, is used to compute convolutional features, which are\n","taken as input in *alexnet.classifier*.\n","\n","The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will\n","output a tensor of shape Nx256x6x6 . (N = batch size).\n","\n","Compute the AlexNet features for each of your training, validation, and test data. Here is an example code\n","snippet showing how you can compute the AlexNet features for some images (your actual code might be\n","different):"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"oX7SjVdB7XAE","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# img = ... a PyTorch tensor with shape [N,3,224,224] containing hand images ...\n","#features = alexnet.features(img)"]},{"cell_type":"markdown","metadata":{"id":"DYcjHg_A7cCM"},"source":["**Save the computed features**. You will be using these features as input to your neural network in Part\n","(b), and you do not want to re-compute the features every time. Instead, run *alexnet.features* once for\n","each image, and save the result."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"TBo1BpL373LX","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["features_path = '/content/drive/My Drive/features'\n","\n","# define dataloader parameters\n","batch_size  = 1\n","num_workers = 1\n","\n","# prepare data loaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                           num_workers=num_workers, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","\n","# save features to folders\n","n=0\n","for img, label in train_loader:\n","  features = alexnet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = features_path + '/train/' + str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(n) + '.tensor')\n","  n+=1\n","\n","n=0\n","for img, label in val_loader:\n","  features = alexnet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = features_path + '/validation/' + str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(n) + '.tensor')\n","  n+=1\n","\n","n=0\n","for img, label in test_loader:\n","  features = alexnet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = features_path + '/test/' + str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(n) + '.tensor')\n","  n+=1\n"]},{"cell_type":"code","source":["train_features_data = torchvision.datasets.DatasetFolder(features_path + '/train', loader=torch.load, extensions=('.tensor'))\n","val_features_data = torchvision.datasets.DatasetFolder(features_path + '/validation', loader=torch.load, extensions=('.tensor'))\n","test_features_data = torchvision.datasets.DatasetFolder(features_path + '/test', loader=torch.load, extensions=('.tensor'))\n","\n","# define dataloader parameters\n","batch_size  = 32\n","num_workers = 1\n","\n","# prepare data loaders\n","train_features_loader = torch.utils.data.DataLoader(train_features_data, batch_size=batch_size,\n","                                           num_workers=num_workers, shuffle=True)\n","val_features_loader = torch.utils.data.DataLoader(val_features_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)\n","test_features_loader = torch.utils.data.DataLoader(test_features_data, batch_size=batch_size,\n","                                          num_workers=num_workers, shuffle=True)"],"metadata":{"id":"0HK88doAs8GY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFWvvhFN73qY"},"source":["### Part (b) - 3 pt\n","Build a convolutional neural network model that takes as input these AlexNet features, and makes a\n","prediction. Your model should be a subclass of nn.Module.\n","\n","Explain your choice of neural network architecture: how many layers did you choose? What types of layers\n","did you use: fully-connected or convolutional? What about other decisions like pooling layers, activation\n","functions, number of channels / hidden units in each layer?\n","\n","Here is an example of how your model may be called:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"oVTuHUeV78-U","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# features = ... load precomputed alexnet.features(img) ...\n","#output = model(features)\n","#prob = F.softmax(output)"]},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self):\n","        super(AlexNet, self).__init__()\n","        self.name = \"AlexNet\"\n","        self.conv1 = nn.Conv2d(256, 64, 2) #the output from features is a 256x6x6\n","        #we just use the first dimension as the input for the conv1 layer.\n","        self.pool = nn.MaxPool2d(1, 1)\n","        self.conv2 = nn.Conv2d(64, 32, 2) #output of conv1 is input of conv2\n","        self.conv3 = nn.Conv2d(32, 16, 2) #same as above\n","        self.fc1 = nn.Linear(16 * 3 * 3, 128) #output from conv3 flattened\n","        self.fc2 = nn.Linear(128, 9) #output is 9 cause 9 classes\n","        #I used the exact same structure as the CNN network above with some\n","        #numbers changed to account for the different dimensions.\n","        #the values for the linear fc1 layer were again calculated\n","        #using the equation in tutorial 3a. The model worked pretty well earlier\n","        #making me believe that it would work well for the AlexNet module as well.\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) #the reLU activation function is\n","        x = self.pool(F.relu(self.conv2(x))) #simple and effective\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 16 * 3 * 3)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        x = x.squeeze(1) # Flatten to [batch_size]\n","        return x"],"metadata":{"id":"9TaIRkCKpt94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVAGuURu7-9q"},"source":["### Part (c) - 5 pt\n","Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your\n","best model only.\n","\n","Note: Depending on how you are caching (saving) your AlexNet features, PyTorch might still be tracking\n","updates to the **AlexNet weights**, which we are not tuning. One workaround is to convert your AlexNet\n","feature tensor into a numpy array, and then back into a PyTorch tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"JCmiH11x7-q1","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#tensor = torch.from_numpy(tensor.detach().numpy())\n","\n","net6 = AlexNet()\n","train_net(train_features_loader, val_features_loader, test_features_loader, net6, num_epochs=9, batch_size=64)\n","model_path = get_model_name(\"AlexNet\", batch_size=64, learning_rate=0.001, epoch=8)\n","plot_training_curve(model_path)"]},{"cell_type":"markdown","metadata":{"id":"hQ2tvqJ68Mqb"},"source":["### Part (d) - 2 pt\n","Report the test accuracy of your best model. How does the test accuracy compare to Part 3(d) without transfer learning?"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"yCp_kFSg8Q2T","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","error, loss = evaluate(net6, test_features_loader, criterion)\n","print(\"Test Error:\", error, \"Test Loss:\", loss)"]},{"cell_type":"markdown","source":["AlexNet had much better accuracy than the CNN. This is because AlexNet has better features that it learned from lots of data. I also tweaked AlexNet a bit to better fit our specific task, which helped improve its performance even more. Overall, I'm really happy with how well the new model worked since it only had an error rate of 7.8%.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"X_FKMvgV1iQb"}},{"cell_type":"code","source":["%%shell\n","jupyter nbconvert --to html \"/content/lab3.ipynb\""],"metadata":{"id":"BIaEw1TG4s0k"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":0}